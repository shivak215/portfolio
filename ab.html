<html>
<div class="intro-header">
      <div class="bg-overlay">
        <div class="container">
  

            <div class="row">
                <div class="col-lg-12">
                    <div class="intro-message">
                        <h1>Welcome to my work</h1>
                        <h3>New Perspectives for handling Non Linearities in a Neural Networks by using Complex Weights and Inverse Operation</h3>
                        <hr class="intro-divider">
                        <ul class="list-inline intro-social-buttons">
                         <p>
<b>NN with complex weights</b>
<br>
<br>
Proposed algorithm works for the training of feed-forward neural network which has inputs, weights, activation functions and an output provided all of them are complex valued. Here we demonstrate the use of back-propagation to solve the problem of complex domain input in neural networks. A model of a generalised multiple neuron with single layer using complex weight is presented in Figure 1.The weights are denoted by w1 = a1+ ib1 through wn = an + ibn.
<br>
<br>
<b> Forward Propagation </b>
<br>
<br>
First layer perceptrons accept multiple inputs, multiplies each input by a weight, sums the weighted inputs, subtracts a threshold(optional or any bias operation ), and passes the result through an out limiting non-linearity.(Kim and Adali 2001). Let us suppose we have a 3 layer neural network. First is input layer second is the hidden layer and the last one is the hidden layer. There are three neurons in the ﬁrst layer, four in the hidden layer and two in the output layer. The shape of each layer matrix can be written as (n.2) where n is the number of the neurons in the respective layer. So after feeding forward and multiplying weights and the corresponding neuron input we get an output weight matrix of the second layer which is 3*4 in shape which is then passed on to the third layer which is the output layer. The input Ini and the weight matrix wi,j denotes the input neurons as well as the weight associated with each neuron in the second layer. For Example: In1 is referred to the value of the ﬁrst input neuron which is complex valued. W1,2 is referred to as the ﬁrst weight which is connecting to the second node of the second layer. So, suppose for example the output can be deﬁned for the two- layer network with two input one output as
<br>
<br>
Y = In1 ×W1,1 + In2 ×W1,2 (1) 
<br>
<br>
So with the help of the output which is complex valued, we calculate the error. The error for the respective input is deﬁned as:
<br><br>Ep = (Yp −Tp)2 (2)
<br><br> • Yp= The output of the neural network
<br><br> • Tp = Ground truth or the expected result 
<br>
<br>
Assuming that the partial derivative of the error function exists is not sufﬁcient. So to check that it should satisfy the Cauchy-Reimann equations. According to the Cauchy-Reimann equation, we have du dx = dv dx (3) dv dx = − du dx (4)
DerivabilityOfErrorFunction
Let us assume that the value of Y and T :
<br><br> Y = x + iy and T = t1 + it2 (5)
<br><br> So putting the values of Y and T in eq (2) we get
<br><br> E = ((x + iy)−(t1 −it2)) (6)
<br><br> E = ((x−t1) + i(y−it2))2 (7) 
<br><br>E = (x−t1)2 −(y−t2)2 + 2i(x−t1)(y−t2) (8)
<br><br>
 So now we have found our U and V which are:
<br><br>U = (x−t1)2−(y−t2)2and V = 2i(x−t1)(y−t2) (9)
<br><br>
 Now applying Cauchy-Reimann test to check the function is derivable or not
<br><br> Ux = 2(x−t1) Vx = 2(y−t2)Uy = −2(y−t2) Vy = 2(x−t1) (10)
<br><br>
 The above derivatives satisfy the Cauchy-Reimann test and now we can write the derivative of our error function which is
<br><br> dE dx = Ux + iVx (11) which can be represented as dE dx = 2(x−t1) + 2i(y−t2) (12) 
<br>After proving the derivability of the Error function we can backpropagate, and with respect to change in the output we can updates the weight accordingly while backpropagating. Backpropagation So now we have the error and which is derivable as we proved it by Cauchy-Reimann equation and which is showed in eq(12). So in order to reduce the error and get precise results Backpropagation is performed in case of deep neural nets or we can say a neural network with multiple hidden layers. While backpropagating we calculate the gradient for each weight with respect to the error and keep on updating the weights of the network in a continuous loop till the time we get the desired output. This whole process of ﬁnding out the gradient of error for each weight and updating is called as learning(Nitta 1993). For backpropagating we are considering a three-layer neural network with 3 neurons in the input,2 in hidden and 2 in the output layer. This model for backpropagating can be applied to any of complex neural network with multiple layers. Procedure The ﬁrst layer is accepting the complex inputs and the neurons for the ﬁrst layer is denoted by Ii, second layer neurons by Hi and output layer by Oi.Every neuron of the adjacent layer is connected to every single neuron having complex weight. The size of the weight matrix for the ﬁrst layer and the second layer is 3*2 whereas the second layer and output layer is 3*2 where each neuron receives an input and sends the output which is denoted by outh1,outh2, here for the hidden layers. A respective three-layer network is shown in the ﬁgure below Let us assume we want to update W1 so in order to get the error with respect to the weight we can write this eqaution using the chain rule of derivatives : dET dw1 = dET doutH1 . doutH1 dW1 (13) Here Etotal is the sum of the error of output from both the output neurons. ET = Eo1 + Eo2 (14)


 <img src=".jpg"  class="img-circle img-responsive" >

</p>

						 
                            
                           
                        </ul>
                    </div>
                </div>
            </div>

        </div>
        <!-- /.container -->
      </div>
    </div>
</html>